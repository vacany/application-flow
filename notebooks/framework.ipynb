{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Models\n",
    "- https://github.com/POSTECH-CVLab/FastPointTransformer\n",
    "    - This also have Minkowsi inside\n",
    "- https://github.com/PRBonn/Mask4D\n",
    "\n",
    "# Inspiration\n",
    "- https://arxiv.org/pdf/2309.16133.pdf\n",
    "- https://arxiv.org/pdf/2305.16404.pdf \n",
    "    - progresive superpoints, unsupervised  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b74df1399ff9e7a"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-27T18:26:33.143386215Z",
     "start_time": "2023-10-27T18:26:25.107148205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "780 ms ± 7.63 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2737667/3743902934.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  init_cluster_ids = torch.tensor(init_cluster_ids, dtype=torch.long, device=device)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir('/home/vacekpa2/4D-RNSFP')\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from pytorch3d.ops.knn import knn_points\n",
    "\n",
    "from data.dataloader import SFDataset4D\n",
    "from vis.deprecated_vis import *\n",
    "from loss.flow import *\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "dataset = SFDataset4D(dataset_type='waymo', n_frames=1)\n",
    "data = dataset.__getitem__(80)\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "pc1 = data['pc1'].to(device)\n",
    "pc2 = data['pc2'].to(device)\n",
    "id_mask1 = data['id_mask1'].to(device)\n",
    "mos1 = data['mos1'].to(device)\n",
    "\n",
    "id_mask1[mos1==False] = 0\n",
    "\n",
    "K = len(torch.unique(id_mask1)) + 5 # number of objects to infer\n",
    "\n",
    "# visualize_points3D(pc1.view(-1,3), id_mask1.view(-1))\n",
    "# os.system('nvidia-smi')\n",
    "# init by dbscan\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "numpy_st_pc = np.concatenate([np.insert(pc1[i].cpu().numpy(), 3, i * 0.20, axis=1) for i in range(pc1.shape[0])])\n",
    "# numpy_st_pc = np.concatenate((pc1[0].cpu().numpy(), data['gt_flow'][0].cpu().numpy()), axis=1)\n",
    "%timeit init_cluster_ids = DBSCAN(eps=0.2, min_samples=3).fit_predict(numpy_st_pc)\n",
    "# init_cluster_ids = KMeans(n_clusters=K, n_init=5).fit_predict(numpy_st_pc)\n",
    "init_cluster_ids = torch.tensor(init_cluster_ids, dtype=torch.long, device=device)\n",
    "visualize_points3D(pc1.view(-1,3), init_cluster_ids.view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Framework for joint trajectory, instance segmentation and flow estimation\n",
    "\n",
    "### Input: Sequence of point clouds as batch of size (B, N, 3) and mask of size (B, N, K) where K is number of objects in the scene.\n",
    "### Output: Flow, instance weights, and trajectories\n",
    "\n",
    "### Features\n",
    "- [x] fitting flow from each frame to the next\n",
    "- [x] Cyclic smoothness\n",
    "- [] fitting instance segmentation from each frame to the next\n",
    "- [ ] model plug and play\n",
    "- [ ] losses inside model\n",
    "\n",
    "### Do not Generate:\n",
    "\n",
    "\n",
    "### Representation\n",
    "\n",
    "### PseudoCode"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d27b0380a52edff"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def cyclic_smoothness(pc1, est_flow, pc2, NN_pc2, NN_forward=None, pc2_smooth=True):\n",
    "    \n",
    "    if NN_forward is None:\n",
    "        # print('not now')\n",
    "        _, NN_forward, _ = knn_points(pc1 + est_flow, pc2, lengths1=None, lengths2=None, K=1, norm=1)\n",
    "    \n",
    "    a = est_flow[0]\n",
    "\n",
    "    ind = NN_forward[0] # more than one?\n",
    "    \n",
    "    if pc1.shape[1] < pc2.shape[1]:\n",
    "        shape_diff = pc2.shape[1] - ind.shape[0] + 1 # one for dummy    # what if pc1 is bigger than pc2?\n",
    "        a = torch.nn.functional.pad(a, (0,0,0, shape_diff), mode='constant', value=0)\n",
    "        a.retain_grad() # padding does not retain grad, need to do it manually. Check it\n",
    "\n",
    "        ind = torch.nn.functional.pad(ind, (0,0,0, shape_diff), mode='constant', value=pc2.shape[1])  # pad with dummy not in orig\n",
    "\n",
    "    # storage of same points\n",
    "    vec = torch.zeros(ind.shape[0], 3, device=pc1.device)\n",
    "\n",
    "    # this is forward flow withnout NN_pc2 smoothness\n",
    "    vec = vec.scatter_reduce_(0, ind.repeat(1,3), a, reduce='mean', include_self=False)\n",
    "\n",
    "    forward_flow_loss = torch.nn.functional.mse_loss(vec[ind[:,0]], a, reduction='none').mean(dim=-1)\n",
    "\n",
    "    if pc2_smooth:\n",
    "        # rest is pc2 smoothness with pre-computed NN\n",
    "        keep_ind = ind[ind[:,0] != pc2.shape[1] ,0]\n",
    "\n",
    "        # znamena, ze est flow body maji tyhle indexy pro body v pc2 a ty indexy maji mit stejne flow.\n",
    "        n = NN_pc2[0, keep_ind, :]\n",
    "\n",
    "        # beware of zeros!!!\n",
    "        connected_flow = vec[n] # N x KSmooth x 3 (fx, fy, fz)\n",
    "\n",
    "        prep_flow = est_flow[0].unsqueeze(1).repeat_interleave(repeats=NN_pc2.shape[-1], dim=1) # correct\n",
    "\n",
    "        # smooth it, should be fine\n",
    "        # print(prep_flow.shape, connected_flow.shape)\n",
    "        flow_diff = prep_flow - connected_flow  # correct operation, but zeros makes problem\n",
    "\n",
    "        occupied_mask = connected_flow.all(dim=2).repeat(3,1,1).permute(1,2,0)\n",
    "\n",
    "        # occupied_mask\n",
    "        per_flow_dim_diff = torch.masked_select(flow_diff, occupied_mask)\n",
    "\n",
    "        # per_point_loss = per_flow_dim_diff.norm(dim=-1).mean()\n",
    "        NN_pc2_loss = (per_flow_dim_diff ** 2).mean()    # powered to 2 because norm will sum it directly\n",
    "\n",
    "    else:\n",
    "        NN_pc2_loss = torch.tensor(0.)\n",
    "\n",
    "    forward_loss = forward_flow_loss.mean() + NN_pc2_loss\n",
    "\n",
    "    return forward_loss#, forward_flow_loss\n",
    "\n",
    "def get_instance(mask_weights):\n",
    "    return torch.nn.functional.softmax(mask_weights, dim=2)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T15:39:21.759471922Z",
     "start_time": "2023-10-27T15:39:21.612117460Z"
    }
   },
   "id": "79c56d9929b880d5"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# fit  flow\n",
    "flow_weights = torch.zeros(pc1.shape, device=device, requires_grad=True)\n",
    "mask_weights = torch.rand((pc1.shape[0], pc1.shape[1], K), device=device, requires_grad=True)\n",
    "\n",
    "# batched fastKNN\n",
    "NN_modules = [FastNN(pc1[i:i+1],pc2[i:i+1], cell_size=0.075) for i in range(pc1.shape[0])]\n",
    "full_ids = [NN_modules[i].full_ids for i in range(pc1.shape[0])]\n",
    "full_ids = torch.stack(full_ids).permute(0,2,3,4,1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T15:39:22.519652710Z",
     "start_time": "2023-10-27T15:39:21.762794611Z"
    }
   },
   "id": "60b88694608a3285"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Flow Loss:  0.0189 Smooth Loss:  0.0019\n",
      "1 Flow Loss:  0.0285 Smooth Loss:  0.0182\n",
      "2 Flow Loss:  0.0264 Smooth Loss:  0.0143\n",
      "3 Flow Loss:  0.0243 Smooth Loss:  0.0091\n",
      "4 Flow Loss:  0.0228 Smooth Loss:  0.0091\n",
      "5 Flow Loss:  0.0218 Smooth Loss:  0.0098\n",
      "6 Flow Loss:  0.0221 Smooth Loss:  0.0095\n",
      "7 Flow Loss:  0.0225 Smooth Loss:  0.0086\n",
      "8 Flow Loss:  0.0221 Smooth Loss:  0.0077\n",
      "9 Flow Loss:  0.0211 Smooth Loss:  0.0068\n",
      "10 Flow Loss:  0.0205 Smooth Loss:  0.0062\n",
      "11 Flow Loss:  0.0215 Smooth Loss:  0.0060\n",
      "12 Flow Loss:  0.0217 Smooth Loss:  0.0059\n",
      "13 Flow Loss:  0.0210 Smooth Loss:  0.0057\n",
      "14 Flow Loss:  0.0201 Smooth Loss:  0.0053\n",
      "15 Flow Loss:  0.0202 Smooth Loss:  0.0049\n",
      "16 Flow Loss:  0.0207 Smooth Loss:  0.0047\n",
      "17 Flow Loss:  0.0206 Smooth Loss:  0.0045\n",
      "18 Flow Loss:  0.0200 Smooth Loss:  0.0043\n",
      "19 Flow Loss:  0.0199 Smooth Loss:  0.0041\n",
      "20 Flow Loss:  0.0203 Smooth Loss:  0.0039\n",
      "21 Flow Loss:  0.0202 Smooth Loss:  0.0038\n",
      "22 Flow Loss:  0.0198 Smooth Loss:  0.0036\n",
      "23 Flow Loss:  0.0198 Smooth Loss:  0.0035\n",
      "24 Flow Loss:  0.0201 Smooth Loss:  0.0033\n",
      "25 Flow Loss:  0.0199 Smooth Loss:  0.0032\n",
      "26 Flow Loss:  0.0196 Smooth Loss:  0.0031\n",
      "27 Flow Loss:  0.0198 Smooth Loss:  0.0030\n",
      "28 Flow Loss:  0.0198 Smooth Loss:  0.0029\n",
      "29 Flow Loss:  0.0197 Smooth Loss:  0.0028\n",
      "30 Flow Loss:  0.0196 Smooth Loss:  0.0027\n",
      "31 Flow Loss:  0.0196 Smooth Loss:  0.0026\n",
      "32 Flow Loss:  0.0196 Smooth Loss:  0.0026\n",
      "33 Flow Loss:  0.0196 Smooth Loss:  0.0025\n",
      "34 Flow Loss:  0.0195 Smooth Loss:  0.0024\n",
      "35 Flow Loss:  0.0195 Smooth Loss:  0.0024\n",
      "36 Flow Loss:  0.0195 Smooth Loss:  0.0023\n",
      "37 Flow Loss:  0.0195 Smooth Loss:  0.0023\n",
      "38 Flow Loss:  0.0194 Smooth Loss:  0.0023\n",
      "39 Flow Loss:  0.0194 Smooth Loss:  0.0022\n",
      "40 Flow Loss:  0.0195 Smooth Loss:  0.0022\n",
      "41 Flow Loss:  0.0194 Smooth Loss:  0.0022\n",
      "42 Flow Loss:  0.0194 Smooth Loss:  0.0021\n",
      "43 Flow Loss:  0.0194 Smooth Loss:  0.0021\n",
      "44 Flow Loss:  0.0194 Smooth Loss:  0.0021\n",
      "45 Flow Loss:  0.0194 Smooth Loss:  0.0021\n",
      "46 Flow Loss:  0.0193 Smooth Loss:  0.0021\n",
      "47 Flow Loss:  0.0193 Smooth Loss:  0.0021\n",
      "48 Flow Loss:  0.0193 Smooth Loss:  0.0021\n",
      "49 Flow Loss:  0.0193 Smooth Loss:  0.0020\n",
      "50 Flow Loss:  0.0192 Smooth Loss:  0.0020\n",
      "51 Flow Loss:  0.0193 Smooth Loss:  0.0020\n",
      "52 Flow Loss:  0.0193 Smooth Loss:  0.0020\n",
      "53 Flow Loss:  0.0193 Smooth Loss:  0.0020\n",
      "54 Flow Loss:  0.0192 Smooth Loss:  0.0020\n",
      "55 Flow Loss:  0.0192 Smooth Loss:  0.0020\n",
      "56 Flow Loss:  0.0192 Smooth Loss:  0.0020\n",
      "57 Flow Loss:  0.0192 Smooth Loss:  0.0020\n",
      "58 Flow Loss:  0.0192 Smooth Loss:  0.0020\n",
      "59 Flow Loss:  0.0192 Smooth Loss:  0.0020\n",
      "60 Flow Loss:  0.0192 Smooth Loss:  0.0020\n",
      "61 Flow Loss:  0.0192 Smooth Loss:  0.0020\n",
      "62 Flow Loss:  0.0191 Smooth Loss:  0.0020\n",
      "63 Flow Loss:  0.0191 Smooth Loss:  0.0020\n",
      "64 Flow Loss:  0.0191 Smooth Loss:  0.0020\n",
      "65 Flow Loss:  0.0191 Smooth Loss:  0.0020\n",
      "66 Flow Loss:  0.0191 Smooth Loss:  0.0020\n",
      "67 Flow Loss:  0.0191 Smooth Loss:  0.0020\n",
      "68 Flow Loss:  0.0191 Smooth Loss:  0.0020\n",
      "69 Flow Loss:  0.0191 Smooth Loss:  0.0020\n",
      "70 Flow Loss:  0.0191 Smooth Loss:  0.0020\n",
      "71 Flow Loss:  0.0191 Smooth Loss:  0.0020\n",
      "72 Flow Loss:  0.0191 Smooth Loss:  0.0020\n",
      "73 Flow Loss:  0.0191 Smooth Loss:  0.0020\n",
      "74 Flow Loss:  0.0191 Smooth Loss:  0.0020\n",
      "75 Flow Loss:  0.0191 Smooth Loss:  0.0020\n",
      "76 Flow Loss:  0.0191 Smooth Loss:  0.0020\n",
      "77 Flow Loss:  0.0190 Smooth Loss:  0.0020\n",
      "78 Flow Loss:  0.0190 Smooth Loss:  0.0020\n",
      "79 Flow Loss:  0.0190 Smooth Loss:  0.0020\n",
      "80 Flow Loss:  0.0190 Smooth Loss:  0.0020\n",
      "81 Flow Loss:  0.0190 Smooth Loss:  0.0020\n",
      "82 Flow Loss:  0.0190 Smooth Loss:  0.0020\n",
      "83 Flow Loss:  0.0190 Smooth Loss:  0.0020\n",
      "84 Flow Loss:  0.0190 Smooth Loss:  0.0020\n",
      "85 Flow Loss:  0.0190 Smooth Loss:  0.0020\n",
      "86 Flow Loss:  0.0190 Smooth Loss:  0.0020\n",
      "87 Flow Loss:  0.0190 Smooth Loss:  0.0020\n",
      "88 Flow Loss:  0.0190 Smooth Loss:  0.0020\n",
      "89 Flow Loss:  0.0190 Smooth Loss:  0.0020\n",
      "90 Flow Loss:  0.0190 Smooth Loss:  0.0020\n",
      "91 Flow Loss:  0.0190 Smooth Loss:  0.0020\n",
      "92 Flow Loss:  0.0190 Smooth Loss:  0.0020\n",
      "93 Flow Loss:  0.0190 Smooth Loss:  0.0020\n",
      "94 Flow Loss:  0.0190 Smooth Loss:  0.0020\n",
      "95 Flow Loss:  0.0190 Smooth Loss:  0.0020\n",
      "96 Flow Loss:  0.0190 Smooth Loss:  0.0020\n",
      "97 Flow Loss:  0.0190 Smooth Loss:  0.0020\n",
      "98 Flow Loss:  0.0190 Smooth Loss:  0.0020\n",
      "99 Flow Loss:  0.0190 Smooth Loss:  0.0020\n",
      "100 Flow Loss:  0.0190 Smooth Loss:  0.0020\n",
      "101 Flow Loss:  0.0190 Smooth Loss:  0.0020\n",
      "102 Flow Loss:  0.0190 Smooth Loss:  0.0020\n",
      "103 Flow Loss:  0.0190 Smooth Loss:  0.0020\n",
      "104 Flow Loss:  0.0190 Smooth Loss:  0.0020\n",
      "105 Flow Loss:  0.0190 Smooth Loss:  0.0020\n",
      "106 Flow Loss:  0.0190 Smooth Loss:  0.0020\n",
      "107 Flow Loss:  0.0190 Smooth Loss:  0.0020\n",
      "108 Flow Loss:  0.0190 Smooth Loss:  0.0020\n",
      "109 Flow Loss:  0.0190 Smooth Loss:  0.0020\n",
      "110 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "111 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "112 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "113 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "114 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "115 Flow Loss:  0.0190 Smooth Loss:  0.0020\n",
      "116 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "117 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "118 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "119 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "120 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "121 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "122 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "123 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "124 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "125 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "126 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "127 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "128 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "129 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "130 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "131 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "132 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "133 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "134 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "135 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "136 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "137 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "138 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "139 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "140 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "141 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "142 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "143 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "144 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "145 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "146 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "147 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "148 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "149 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "150 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "151 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "152 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "153 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "154 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "155 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "156 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "157 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "158 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "159 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "160 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "161 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "162 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "163 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "164 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "165 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "166 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "167 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "168 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "169 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "170 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "171 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "172 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "173 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "174 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "175 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "176 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "177 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "178 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "179 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "180 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "181 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "182 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "183 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "184 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "185 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "186 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "187 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "188 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "189 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "190 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "191 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "192 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "193 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "194 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "195 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "196 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "197 Flow Loss:  0.0189 Smooth Loss:  0.0021\n",
      "198 Flow Loss:  0.0189 Smooth Loss:  0.0020\n",
      "199 Flow Loss:  0.0189 Smooth Loss:  0.0021\n",
      "200 Flow Loss:  0.0189 Smooth Loss:  0.0021\n",
      "201 Flow Loss:  0.0189 Smooth Loss:  0.0021\n",
      "202 Flow Loss:  0.0189 Smooth Loss:  0.0021\n",
      "203 Flow Loss:  0.0189 Smooth Loss:  0.0021\n",
      "204 Flow Loss:  0.0189 Smooth Loss:  0.0021\n",
      "205 Flow Loss:  0.0189 Smooth Loss:  0.0021\n",
      "206 Flow Loss:  0.0189 Smooth Loss:  0.0021\n",
      "207 Flow Loss:  0.0189 Smooth Loss:  0.0021\n",
      "208 Flow Loss:  0.0189 Smooth Loss:  0.0021\n",
      "209 Flow Loss:  0.0189 Smooth Loss:  0.0021\n",
      "210 Flow Loss:  0.0189 Smooth Loss:  0.0021\n",
      "211 Flow Loss:  0.0189 Smooth Loss:  0.0021\n",
      "212 Flow Loss:  0.0189 Smooth Loss:  0.0021\n",
      "213 Flow Loss:  0.0189 Smooth Loss:  0.0021\n",
      "214 Flow Loss:  0.0189 Smooth Loss:  0.0021\n",
      "215 Flow Loss:  0.0189 Smooth Loss:  0.0021\n",
      "216 Flow Loss:  0.0189 Smooth Loss:  0.0021\n",
      "217 Flow Loss:  0.0189 Smooth Loss:  0.0021\n",
      "218 Flow Loss:  0.0189 Smooth Loss:  0.0021\n",
      "219 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "220 Flow Loss:  0.0189 Smooth Loss:  0.0021\n",
      "221 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "222 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "223 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "224 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "225 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "226 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "227 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "228 Flow Loss:  0.0189 Smooth Loss:  0.0021\n",
      "229 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "230 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "231 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "232 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "233 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "234 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "235 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "236 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "237 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "238 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "239 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "240 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "241 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "242 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "243 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "244 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "245 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "246 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "247 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "248 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "249 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "250 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "251 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "252 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "253 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "254 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "255 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "256 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "257 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "258 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "259 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "260 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "261 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "262 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "263 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "264 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "265 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "266 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "267 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "268 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "269 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "270 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "271 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "272 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "273 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "274 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "275 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "276 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "277 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "278 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "279 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "280 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "281 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "282 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "283 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "284 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "285 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "286 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "287 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "288 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "289 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "290 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "291 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "292 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "293 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "294 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "295 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "296 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "297 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "298 Flow Loss:  0.0188 Smooth Loss:  0.0021\n",
      "299 Flow Loss:  0.0188 Smooth Loss:  0.0021\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam([flow_weights, mask_weights], lr=0.008)\n",
    "\n",
    "# smoothness for batch size\n",
    "SmoothModule = SmoothnessLoss(pc1=pc1, pc2=pc2, K=12, max_radius=1, pc2_smooth=True)\n",
    "NN_pc2 = SmoothModule.NN_pc2\n",
    "\n",
    "\n",
    "for e in range(300):\n",
    "    dist_list = []\n",
    "    bs = pc1.shape[0]\n",
    "    for t in range(pc1.shape[0]):\n",
    "        dist, NN_index = NN_modules[t](pc1[t:t+1], flow_weights[t:t+1], pc2[t:t+1])\n",
    "        dist_list.append(dist)\n",
    "    # bs only 1\n",
    "    \n",
    "    # cyclic_smooth_loss = cyclic_smoothness(pc1, flow_weights, pc2, NN_pc2, NN_forward=NN_index.unsqueeze(0).unsqueeze(2), pc2_smooth=True)\n",
    "    \n",
    "    loss_dist = torch.cat(dist_list)\n",
    "    smooth_loss = SmoothModule(pc1, flow_weights, pc2)\n",
    "    mask_smooth_loss = SmoothModule(pc1, mask_weights, pc2)\n",
    "    \n",
    "    # pseudo_mask_loss = \n",
    "    # Art loss\n",
    "    static_mask = flow_weights.norm(dim=-1) < 0.05\n",
    "    # todo reweight art loss dynamically?\n",
    "    \n",
    "    label_loss = - (mask_weights[:, ..., 0] * static_mask).softmax(dim=1).mean() + (mask_weights[:, ..., 0] * ~static_mask).softmax(dim=1).mean() \n",
    "\n",
    "    # loss = loss_dist.mean() + smooth_loss.mean()\n",
    "    \n",
    "    loss = bs * loss_dist.mean() + bs * smooth_loss.mean() + mask_smooth_loss.mean() + label_loss.mean() #+ cyclic_smooth_loss.mean() \n",
    "    # loss = smooth_loss.mean()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    print(e, 'Flow Loss: ', f\"{loss_dist.mean().item():.4f}\", 'Smooth Loss: ', f\"{smooth_loss.mean().item():.4f}\") #,'Cyclic Smooth Loss: ', f\"{cyclic_smooth_loss.mean().item():.4f}\")\n",
    "\n",
    "\n",
    "vis_pc1 = pc1.view(-1, 3).cpu()\n",
    "vis_pc2 = pc2.view(-1, 3).cpu()\n",
    "vis_flow = flow_weights.view(-1, 3).cpu()\n",
    "\n",
    "visualize_flow3d(vis_pc1, vis_pc2, vis_flow)\n",
    "pred_inst_ids = torch.argmax(mask_weights, dim=2).view(-1).cpu()\n",
    "# visualize_points3D(vis_pc1, pred_inst_ids)\n",
    "# visualize_points3D(vis_pc1, pred_inst_ids > 0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T15:40:48.294854881Z",
     "start_time": "2023-10-27T15:40:35.861546588Z"
    }
   },
   "id": "f6cefd90706f23ef"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "visualize_flow3d(pc1[2], pc2[2], flow_weights[2])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T15:40:13.837614035Z",
     "start_time": "2023-10-27T15:40:13.334995400Z"
    }
   },
   "id": "74473c7c386d5284"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199958, 12, 3])\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "est_flow = flow_weights\n",
    "NN_idx = SmoothModule.NN_pc1\n",
    "\n",
    "\n",
    "est_flow_neigh = est_flow.reshape(-1, 3)[NN_idx.reshape(-1, 12)]\n",
    "print(est_flow_neigh.shape)\n",
    "flow_diff = est_flow_neigh[:, :1, :] - est_flow_neigh[:, 1:, :]\n",
    "# est_flow_neigh = est_flow_neigh[:, 1:, :]   # drop identity to ease computation\n",
    "\n",
    "smooth_flow_loss = flow_diff.norm(dim=2).mean()\n",
    "print(smooth_flow_loss)\n",
    "\n",
    "# torch.index_select(est_flow, 1, NN_idx).shape\n",
    "# index est_flow with NN_idx to get points\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T15:36:15.765325859Z",
     "start_time": "2023-10-27T15:36:15.277542230Z"
    }
   },
   "id": "cb1e54ec2855ca0d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
